## 1. Data Collection 
### 1.1. Import Ncessary Library
# import library
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
### 1.2. Load the Dataset
# Load the dataset
file_path = '../data/MachineLearningRating_v3.txt'
df = pd.read_csv(file_path, sep='|')
# Display basic information
print("Data Overview")
df.head()
## 2. Explanatory Data Analysis (EDA)
df.shape
df.info()
df.columns
for column in df.columns:
    print(f"Unique values for column '{column}':")
    print(df[column].unique())
    print()  

df.dtypes
df.describe()
# Check for missing values
print(df.isnull().sum())
# Calculate the sum of missing values for each column
missing_counts = df.isnull().sum()

# Filter to get only columns with missing values
columns_with_missing = missing_counts[missing_counts > 0]

# Create a DataFrame for plotting
missing_values_df = pd.DataFrame({
    'Column': columns_with_missing.index,
    'Missing Values': columns_with_missing.values
})

# Plot the table
fig, ax = plt.subplots(figsize=(10, 6))  # Set the size of the plot
ax.axis('off')  # Hide the axes

# Create the table
table = ax.table(
    cellText=missing_values_df.values,
    colLabels=['Column', 'Missing Values'],
    cellLoc='center',
    loc='center',
    cellColours=[['lightgrey'] * 2] * len(missing_values_df),
    colColours=['#d9d9d9'] * 2
)

# Add a title
plt.title('Columns with Missing Values', fontsize=14)

# Display the table
plt.show()
import seaborn as sns
sns.heatmap(df.isnull(), cbar=False)
### 2.2. Univariate Analysis
#### a. Distribution of Variables
# Histograms for numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
numerical_cols
numerical_cols.isnull().sum()
# Set up the matplotlib figure
plt.figure(figsize=(16, 12))

# Plot histograms
for i, col in enumerate(numerical_cols):
    plt.subplot(4, 4, i + 1)  # Adjust 4x4 to fit the number of numerical columns
    sns.histplot(df[col].dropna(), bins=20, kde=False)  # kde=True to add Kernel Density Estimate
    plt.title(col)
    plt.xlabel('Value')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
#### Categorical Columns
# Bar charts for categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols
# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Plot bar charts for categorical columns
for col in categorical_cols:
    df[col].value_counts().plot(kind='bar', figsize=(10, 5))
    plt.title(f'Distribution of {col}')
    plt.show()
### 2.2. Bivariate and Multivariate Analysis
Correlations and Associations
# Scatter plot of TotalPremium vs. TotalClaims
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='TotalPremium', y='TotalClaims')
plt.title('TotalPremium vs. TotalClaims')
plt.xlabel('TotalPremium')
plt.ylabel('TotalClaims')
plt.show()
# Correlation matrix
correlation_matrix = df[['TotalPremium', 'TotalClaims', 'PostalCode']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()
## 3. Preprocessing
3.1 Data Comparison
import matplotlib.pyplot as plt
import numpy as np

# Set a focused x-axis range (e.g., focusing on PostalCodes between 0 and 3000)
postalcode_subset = postalcode_summary_scaled[(postalcode_summary_scaled.index >= 0) & (postalcode_summary_scaled.index <= 300000)]

# Set larger figure size
plt.figure(figsize=(15, 8))

# Plot each metric for the subset of PostalCodes
for column in postalcode_subset.columns:
    plt.plot(postalcode_subset.index, postalcode_subset[column], marker='o', label=column)

# Rotate x-axis labels to prevent overlap
plt.xticks(np.arange(0, len(postalcode_subset.index), step=20), rotation=90, ha='right')

# Set title, labels, and legend
plt.title('Standardized Average Premium, Sum Insured, and Claims by Postal Code (Subset)', fontsize=16)
plt.xlabel('PostalCode', fontsize=14)
plt.ylabel('Standardized Values', fontsize=14)
plt.legend(title='Metrics', fontsize=12)

# Set a y-axis range to clip outliers and improve visualization (e.g., y-axis between -2 and 5)
plt.ylim(-2, 5)

# Add grid
plt.grid(True)

# Adjust layout to prevent overlap
plt.tight_layout()

plt.show()

3.2. Data Cleaning
# Calculate the percentage of missing values for each column
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

# Combine the missing values and percentages into a single DataFrame
missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})

# Filter out columns with missing values only
missing_data = missing_data[missing_data['Missing Values'] > 0]

# Display the missing values and their percentage
print(missing_data.sort_values(by='Percentage', ascending=False))

3.2.1 Handling Missing Values
# Drop columns with very high missing values (over 50%)
cols_to_drop = ['NumberOfVehiclesInFleet', 'CrossBorder', 'CustomValueEstimate', 
                'Converted', 'Rebuilt', 'WrittenOff']
df = df.drop(columns=cols_to_drop)

# Impute missing values for categorical columns with the mode
categorical_cols = ['NewVehicle', 'AccountType', 'Bank', 'VehicleType', 
                    'make', 'Model', 'mmcode', 'bodytype']
for col in categorical_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Impute missing values for numerical columns with the mean
numerical_cols = ['cubiccapacity', 'kilowatts', 'Cylinders', 'NumberOfDoors']
for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)

# Drop rows with missing values in the 'CapitalOutstanding' column
df.dropna(subset=['CapitalOutstanding'], inplace=True)

# Verify if there are any remaining missing values
print(df.isnull().sum())

3.2.2 Removing Redundant and irelevant Data
# Drop columns with excessive missing values or those irrelevant to analysis
columns_to_drop = [
    'NewVehicle', 'VehicleType', 'make', 'Model', 
    'bodytype', 'Cylinders', 'cubiccapacity', 'RegistrationYear', 
    'VehicleIntroDate', 'kilowatts', 'NumberOfDoors'
]
df = df.drop(columns=columns_to_drop)

# Drop duplicate rows if any
df = df.drop_duplicates()

# Reset index after dropping duplicates
df = df.reset_index(drop=True)

3.2.3 Outlier Detection
df.columns
# Update numerical columns list based on actual columns in DataFrame
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
print("Updated Numerical Columns List:")
print(numerical_cols)


import matplotlib.pyplot as plt
import seaborn as sns

for col in numerical_cols:
    plt.figure(figsize=(10, 5))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')
    plt.xlabel(col)
    plt.show()

3.3 Visualization
import matplotlib.pyplot as plt

# Group by PostalCode and calculate the mean TotalPremium, sorting by the values
premium_by_postalcode = df.groupby('PostalCode')['TotalPremium'].mean().sort_values()

# Create the bar plot with adjusted figure size and font size
plt.figure(figsize=(10, 15))  # Increase height to fit labels
premium_by_postalcode.plot(kind='barh', color='skyblue')

# Titles and labels
plt.title('Average Total Premium by Postal Code')
plt.xlabel('Average Premium')
plt.ylabel('PostalCode')

# Adjust the font size for y-axis labels
plt.yticks(fontsize=8)  # Reduce font size if needed

plt.tight_layout()  # Ensures everything fits well in the figure
plt.show()

# Scatter plot to explore correlation between premium and claims
plt.figure(figsize=(10, 6))
sns.regplot(x='TotalPremium', y='TotalClaims', data=df, scatter_kws={'alpha':0.3})
plt.title('Correlation Between Premium and Claims')
plt.xlabel('Total Premium')
plt.ylabel('Total Claims')
plt.show()

